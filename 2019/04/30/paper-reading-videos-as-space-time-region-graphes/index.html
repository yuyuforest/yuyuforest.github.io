<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.4.2" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=6.4.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=6.4.2">


  <link rel="mask-icon" href="/images/logo.svg?v=6.4.2" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.4.2',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="原文地址 本篇博客分为两个部分：  原文翻译：参考chrome的网页翻译，对原文逐句进行了翻译。 论文笔记：总结论文的主要思想。">
<meta name="keywords" content="深度学习,视频分类">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习论文阅读 | Videos as Space-Time Region Graphes">
<meta property="og:url" content="https://yuyuforest.github.io/2019/04/30/paper-reading-videos-as-space-time-region-graphes/index.html">
<meta property="og:site_name" content="yuyu&#39;s forest">
<meta property="og:description" content="原文地址 本篇博客分为两个部分：  原文翻译：参考chrome的网页翻译，对原文逐句进行了翻译。 论文笔记：总结论文的主要思想。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://yuyuforest.github.io/2019/04/30/paper-reading-videos-as-space-time-region-graphes/figure-1.png">
<meta property="og:image" content="https://yuyuforest.github.io/2019/04/30/paper-reading-videos-as-space-time-region-graphes/figure-2.png">
<meta property="og:image" content="https://yuyuforest.github.io/2019/04/30/paper-reading-videos-as-space-time-region-graphes/table-1.png">
<meta property="og:image" content="https://yuyuforest.github.io/2019/04/30/paper-reading-videos-as-space-time-region-graphes/figure-3.png">
<meta property="og:image" content="https://yuyuforest.github.io/2019/04/30/paper-reading-videos-as-space-time-region-graphes/figure-4.png">
<meta property="og:image" content="https://yuyuforest.github.io/2019/04/30/paper-reading-videos-as-space-time-region-graphes/table-2.png">
<meta property="og:image" content="https://yuyuforest.github.io/2019/04/30/paper-reading-videos-as-space-time-region-graphes/figure-5.png">
<meta property="og:image" content="https://yuyuforest.github.io/2019/04/30/paper-reading-videos-as-space-time-region-graphes/table-3.png">
<meta property="og:image" content="https://yuyuforest.github.io/2019/04/30/paper-reading-videos-as-space-time-region-graphes/table-4.png">
<meta property="og:updated_time" content="2019-05-01T11:59:39.916Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习论文阅读 | Videos as Space-Time Region Graphes">
<meta name="twitter:description" content="原文地址 本篇博客分为两个部分：  原文翻译：参考chrome的网页翻译，对原文逐句进行了翻译。 论文笔记：总结论文的主要思想。">
<meta name="twitter:image" content="https://yuyuforest.github.io/2019/04/30/paper-reading-videos-as-space-time-region-graphes/figure-1.png">






  <link rel="canonical" href="https://yuyuforest.github.io/2019/04/30/paper-reading-videos-as-space-time-region-graphes/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>深度学习论文阅读 | Videos as Space-Time Region Graphes | yuyu's forest</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">yuyu's forest</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />标签</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />分类</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://yuyuforest.github.io/2019/04/30/paper-reading-videos-as-space-time-region-graphes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yuyu">
      <meta itemprop="description" content="鱼鱼爱吃鱼">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yuyu's forest">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">深度学习论文阅读 | Videos as Space-Time Region Graphes
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-30 15:25:25" itemprop="dateCreated datePublished" datetime="2019-04-30T15:25:25+08:00">2019-04-30</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-01 19:59:39" itemprop="dateModified" datetime="2019-05-01T19:59:39+08:00">2019-05-01</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><a href="https://www.groundai.com/project/videos-as-space-time-region-graphs/" target="_blank" rel="noopener">原文地址</a></p>
<p>本篇博客分为两个部分：</p>
<ul>
<li>原文翻译：参考chrome的网页翻译，对原文逐句进行了翻译。</li>
<li>论文笔记：总结论文的主要思想。</li>
</ul>
<a id="more"></a>
<p>翻译仅作参考。因本人水平有限，如翻译中有错漏之处，敬请谅解，同时欢迎发邮件给我纠正。</p>
<p>关于翻译，阅读之前有几点注意事项：</p>
<ul>
<li>有一些我不了解、因此可能翻译错误的专业术语，我会标下划线。这部分术语我会把它的中文翻译和英文原词一并列出来，或者直接只用英文原词。</li>
<li>提到的文献编号，我不会给出具体名称，详情请查阅原文。</li>
</ul>
<h1 id="原文翻译"><a class="header-anchor" href="#原文翻译"> </a>原文翻译</h1>
<p><img src="/2019/04/30/paper-reading-videos-as-space-time-region-graphes/figure-1.png" alt="图1"></p>
<blockquote>
<p>图1：您如何识别打开书本等简单操作？我们认为对行动的理解需要外观建模，但也要捕捉时间动态（书的形状如何变化）和功能关系。我们建议将视频表示为时空区域图，然后是图形卷积以进行推理。</p>
</blockquote>
<h2 id="abstract"><a class="header-anchor" href="#abstract"> </a>Abstract</h2>
<p>人类如何认识到“开书”的行为？我们认为有两个重要的线索：将形状随时间的变化建模、将人与物之间的功能关系建模。在本论文中，我们建议将视频表示为捕捉这两个重要线索的时空区域图。我们的图节点由<u>长程long range</u>视频中不同帧的对象区域定义。这些节点通过两种类型的关系连接起来：（1）相似性关系，它捕捉相关对象之间的<u>长程依赖性long range dependencies</u>；（2）时空关系，它捕捉邻近对象之间的相互作用。我们通过图形卷积网络研究这种图表示。我们在Charades和Something-Something数据集上实现了最先进的结果。特别是Charades，当我们的模型应用于复杂的环境时，获得了巨大的、4.4%的增益。</p>
<h2 id="introduction"><a class="header-anchor" href="#introduction"> </a>Introduction</h2>
<p>考虑一个简单的动作，比如“打开一本书”，如图1所示。当我们人类看到一系列时，我们可以很容易地识别出动作类别；然而，我们当前的视觉系统（有数百层3D卷积）仍在这个简单的任务上挣扎。这是为什么？目前的视频识别框架缺少了什么？</p>
<p>我们先来看一下图1所示的序列。人类如何识别出视频中的动作对应于“打开一本书”？我们认为，要解决这个问题，有两个关键因素。首先，书本的形状和它随时间的变化（即，对象状态由闭合变成打开），是一个至关重要的线索。要利用这个提示，需要在时间上将【跨时间的书本区域】和【把动作建模为<u>转换transformations</u>】这二者联系起来。但仅仅对对象的<u>时间动态temporal dynamics</u>进行建模是不够的。在与人或其他对象互相作用后，对象的状态会发生变化。因此，为了进行动作识别，我们还需要模拟人-物、物-物的相互作用。</p>
<p>但是，我们现有的深度学习方法没能抓住这两个关键因素。例如，基于双流ConvNets的最先进的方法，仍在学习对基于个别视频帧或局部运动矢量之上的动作进行分类。局部运动显然无法对形状变化的动态建模。为了克服这个限制，最近的工作还关注利用回归神经网络、3D卷积来对长期的时间信息进行建模。但是，所有这些框架都关注从整个场景中提取的特征，无法捕捉到长程的、时间上的依赖性（转换），或基于区域的关系。事实上，大多数动作都是根据背景信息进行分类，而不是像文献[11]中所观察到的那样，捕捉关键对象（例如，“打开一本书”中的书本）。</p>
<p>另一方面，也有一些团队为对人-物或物-物的交互进行具体建模而作出了努力。这个方向最近又被重新探索，伴随着ConvNets，来努力提升对象检测、视觉关系检测、动作识别等等。但是，关系推理仍然在静态图像中执行，未能捕捉到这些交互动作的时间动态。因此，这些方法很难捕捉到对象状态随时间的变化，以及这些变化的原因和影响。</p>
<p>在本文中，我们建议通过基于图的推理框架对人-物和物-物进行长程的、时间的建模。不同于关注局部运动矢量的现有方法，我们的模型采用长程视频序列（例如，超过100帧或5秒）。我们将输入视频表示为**<u>时空区域图space-time region graph</u><strong>，图中的每个节点代表视频中令人感兴趣的区域。区域节点通过两种类型的边连接：外观相似性和时空接近度。具体而言，（i）</strong><u>相似关系similarity relations</u><strong>：外观相似或语义相关的区域连接在一起。通过相似关系，我们可以模拟同一对象的状态如何变化，和任何帧、任何两个对象之间的长程的依赖。（ii）</strong><u>时空关系spatial-temporal relations</u>**：在空间上重叠、在时间上接近的对象会通过这些边连接到一起。通过时空关系，我们可以捕捉到邻近对象之间的相互作用，以及对象状态变化的时间顺序。</p>
<p>给出图形表示，我们在图上进行推理，并通过应用图形卷积网络（GCN）推断出动作。我们在具有挑战性的Charades和20BN-Something-Something数据集中进行实验。两个数据集都极具挑战性，因为由场景中的背景和人类或物体的2D外观无法轻易推断出动作。我们的模型显示了对动作识别的最新结果的显著改进。特别是在Charades数据库中，我们获得了4.4%的提升。</p>
<p>我们的贡献包括：（a）关于在长程视频中的对象之间的多样关系的新颖的图形表示；（b）一个可用于多个关系边的推理的图形卷积网络；（c）最先进的表现，在复杂环境中的动作识别这方面获得重大收益。</p>
<h2 id="related-work"><a class="header-anchor" href="#related-work"> </a>Related Work</h2>
<p>**视频理解模型。**时空推理是视频理解和动作识别领域的核心研究领域之一。但是，大多数早期工作都集中在使用时空的外观特征。例如，有很大一部分努力花费在手动设计视频特征。一些人工设计的功能，比如<u>Improved Dense Trajectory</u>（IDT），仍然广泛应用，并在不同的视频相关任务中显示出很有竞争力的结果。然而，最近的研究已经集中在从视频数据学习深度表示，取代了设计人工制作的特征。最受欢迎的模型之一是双流ConvNets，其中时间信息是由具有10个光流帧作为输入的网络来建模的（&lt; 1 second）。为了更好地对长期的信息建模，已经有大量的工作集中在使用回归神经网络（RNNs）和3D的ConvNets。但是，这些框架专注于从整个场景中提取特征，并且很难在空间和时间上模拟不同对象实例之间的关系。</p>
<p>**视觉关系。**关于两两关系的推理，已被证明是对各种计算机视觉任务非常有帮助的。例如，通过对人-物相互作用进行建模，可以显著改善杂乱场景中的对象检测。最近，视觉关系，连同深度网络，已被广泛地应用于对视觉问题的回答、对象识别和<u>直观的物理学intuitive physics</u>。在动作识别这方面，已经有大量的努力花在对人-物和物-物的两两关系的建模。然而，这些工作中的交互推理框架关注的是静态图像，并且时间信息通常是由在图像级特征上的RNN网络来建模。因此，这些方法仍然无法捕捉到某个对象的状态如何随时间变化。</p>
<p>最近，有团队尝试在<u>非局部神经网络Non-local Neural Networks</u>中对空间和时间中的两两关系进行建模。但是，非局部运算符应用于特征空间中的每个像素（从低层到高层），而我们的推理是基于具有对象级别的特征的图像。此外，非局部运算符不处理任何时间排序信息，而这是在我们的时空关系中明确建模的。</p>
<p>**图形模型。**图像和视频中的长程关系通常由图形模型来捕捉。一种流行的方向是，使用<u>Conditional Random Fields</u>（CRF）。在深度学习的上下文中，特别是对于语义分割，该CRF模型通常通过执行<u>mean-field inference</u>来应用于ConvNets的输出。而最近，不同的更简单的、基于前馈图的神经网络被提出来了。在本文中，我们应用了图形卷积网络（GCN），它最初被提出用于自然语言处理。我们的GCN是通过堆叠具有相似性关系和时空关系的多层图形卷积而构建的。GCN的输出是每个对象节点的更新的特征，可用于执行分类。</p>
<p>我们的工作也与包括对象线索、对象图形模型的视频识别有关。例如，Structural-RNN提出了对视频识别任务的对象（时间上相邻）之间的时空关系进行建模。与这些成果不同，我们的时空图表示不仅编码了局部关系，而且编码了跨空间、跨时间的任何一对对象之间的长程依赖关系。通过使用具有长程关系的图形卷积，它实现了在对象的起始状态和结束状态之间有效的消息传递。这种全局推理图框架提供了对最先进技术的显著提升。</p>
<h2 id="overview"><a class="header-anchor" href="#overview"> </a>Overview</h2>
<p><img src="/2019/04/30/paper-reading-videos-as-space-time-region-graphes/figure-2.png" alt="图2"></p>
<blockquote>
<p>图2：模型概述。我们的模型使用3D卷积提取视觉特征，然后RoIAlign为每个建议对象提取$d$维度特征。这些特征作为图形卷积网络的输入，基于时空边进行信息传播。最后，提取$d$维度特征并将其附加到另一个$d$维度视频特征，以执行分类。</p>
</blockquote>
<p>我们的目标是将视频表示为对象图，并在图表上执行推理以进行动作识别。我们的模型概述如图2所示。我们模型的输入参数包括视频帧（5秒以上）的长片段，并将其转发到一个3D卷积神经网络。该3D卷积网络的输出是一个尺寸为$T×H×W×d$的特征图，其中$T$表示时间维度，$H×W$表示空间维度，$d$表示通道数。</p>
<p>除了提取视频特征以外，我们还应用<u>Region Proposal Network</u>（RPN）来提取对象边界框（为了简单起见，我们没有将图2中的RPN可视化）。对$T$个特征帧中每一帧的每个边界框，我们应用RoIAlign提取它们的特征。注意，RoIAlign独立地应用于每一个特征帧。每个对象的特征向量有$d$维（首先对齐到$7×7×d$，然后最大池化为$1×1×d$）。我们将对象数表示为$N$，因此在$RoIAlign$之后特征尺寸为$N×d$。</p>
<p>我们现在构造一个图，这个图包含$N$个节点，这些节点对应于在$T$个帧上聚合的$N$个建议对象。图中主要有两种关系：相似关系和时空关系。为简单起见，我们将这个大图分解为两个具有相同节点但关系不同的子图：相似图和时空图。</p>
<p>通过图形表示，我们应用图形卷积网络（GCN）来进行推理。GCN的输出与输入特征有相同的尺寸$N×d$。我们对所有对象节点执行平均池化，以获得一个$d$维度特征。除了GCN特征之外，我们还对整个视频表示（$T×H×W×d$​）执行平均池化，以获得一个同样是$d$维度的特征，作为全局特征。然后这两个特征被连接到一起，以进行视频级别的分类。</p>
<h2 id="graph-representation-in-videos"><a class="header-anchor" href="#graph-representation-in-videos"> </a>Graph Representation in Videos</h2>
<p>在本节中，我们将首先介绍使用3D卷积网络的特征提取过程，然后描述相似图和时空图的构造。</p>
<p><img src="/2019/04/30/paper-reading-videos-as-space-time-region-graphes/table-1.png" alt="表1"></p>
<blockquote>
<p>表1：我们的基准ResNet-50 I3D模型。我们使用$T×H×W$来表示滤波器内核和3D输出特征图的维数。对于滤波器内核，我们还有$T×H×W$之后的通道数。输入为$32×224×224$维，残差块显示在括号中。</p>
</blockquote>
<h3 id="video-representation"><a class="header-anchor" href="#video-representation"> </a>Video Representation</h3>
<p>**视频骨干模型。**给定一段长视频片段（大约5秒钟），我们从中采样32个视频帧，每两帧之间具有相同的持续时间。我们通过3D卷积网络提取这些帧的特征。表1显示了基于ResNet-50架构（受文献[]58的启发）的骨干模型。该模型将输入视为具有$224×224$维度的32个视频帧，而最后一个卷积层的输出是$16×14×14$的特征图（即，时间维度内有16个帧，而空间维度为$14×14$）。本文中的基准方法采用相同的体系结构，并且通过在最终卷积特征上使用全局平均池化、再附加一个全连接层，来简单地执行分类。</p>
<p>这个骨干模型被称为<u>充气的3D卷积网络Inflated 3D ConvNet</u>（I3D）。可以通过使初始化期间的核膨胀，把一个2D的ConvNet转换为3D的ConvNet。也就是说，通过将权重复制$t$次、缩小为原来的$1/t$，可以使2D的$k×k$的内核膨胀为3D的$t×k×k$的内核。可以参考文献[48,8,58]来获取关于初始化的更多细节。</p>
<p>**<u>区域提议网络Region Proposal Network</u>。**在文献[79, 82]中，我们应用Region Proposal Network（RPN）来在每一个视频帧上生成感兴趣的对象边界框。更具体地说，我们使用有ResNet-50骨干模型的RPN，和FPN。RPN使用MSCOCO对象检测数据集进行预训练，并且RPN和我们的I3D视频骨干模型之间没有权重共享。注意，RPN提取的边界框是<u>类不可知class-agnostic</u>的。</p>
<p>为了在最后一个卷积层之上提取对象特征，我们将16个输入RGB帧（从I3D的32个输入帧中采样，采样率为每2帧采样1帧）的边界框投影到16个输出特征帧。通过视频特征和投影的边界框，我们应用RoIAlign来提取每个建议对象的特征。在RoIAlign中，每个输出帧都是独立处理的。RoIAlign为每个对象生成了$7×7×d$的输出特征，然后这个输出特征被最大池化为$1×1×d$的。</p>
<h3 id="similarity-graph"><a class="header-anchor" href="#similarity-graph"> </a>Similarity Graph</h3>
<p><img src="/2019/04/30/paper-reading-videos-as-space-time-region-graphes/figure-3.png" alt="图3"></p>
<blockquote>
<p>图3：相似图$G^{sim}$。上图显示我们的相似图不仅捕捉到了视觉空间中的相似性，还捕捉到了相关性（功能空间中的相似性）。查询框以橙色显示，最近的邻居以蓝色显示。透明的绿色框是其他未选择的建议对象。</p>
</blockquote>
<p>我们测量特征空间中的对象之间的相似性以构造相似图。在此图中，我们将成对的语义相关对象连接在一起。更具体地说，我们将在两个实例之间有一条<u>高置信度的边high confidence edge</u>，这两个实例可能是：（i）不同视频帧中不同状态下的同一对象，或（ii）在识别动作的方面十分相关。注意，相似性边的计算会在任何一对对象之间发生。</p>
<p>形式上，假设我们拥有视频中所有建议对象的特征$X={x_1,x_2,\dots,x_N}$，其中$N$表示建议对象的数量，并且每个建议对象的特征$x_i$是$d$维向量。每两个建议对象之间的相似性或<u>亲和度affinity</u>可表示为：</p>
<p>$$F(x_i,x_j)=\phi(x_i)^T\phi’(x_j)\tag{1}$$</p>
<p>其中，$\phi$和$\phi’$代表原始特征的两种不同变换。更具体地说，我们有$\phi(X)=wx$和$\phi’(x)=w’x$。参数$w$和$w’$都是$d×d$维度权重，可以通过反向传播来学习。通过添加变换权重$w$和$w’$，我们不仅可以学习跨帧的同一对象实例的不同状态之间的相关性，还可以学习不同对象之间的关系。我们将图3中的建议对象的最近邻居可视化。在第一个例子中，我们可以看到笔记本电脑的最近邻居不仅包括其他帧中的笔记本电脑实例，还包括正在操作它的人。</p>
<p>在用方程$(1)$计算了<u>亲和度矩阵affinity matrix</u>之后，我们对矩阵的每一行执行归一化，使得连接到一个建议对象$i$的所有边的值总和为1。受近期文献[85,58]的启发，我们为归一化采用了softmax功能，以获得相似图，</p>
<p>$$G<sup>{sim}_{ij}=\dfrac{expF(x_i,x_j)}{\sum</sup>N_{j=1}expF(x_i,x_j)}\tag{2}$$</p>
<p>将归一化了的$G^{sim}$作为表示相似图的邻接矩阵。</p>
<h3 id="spatial-temporal-graph"><a class="header-anchor" href="#spatial-temporal-graph"> </a>Spatial-Temporal Graph</h3>
<p><img src="/2019/04/30/paper-reading-videos-as-space-time-region-graphes/figure-4.png" alt="图4"></p>
<blockquote>
<p>图4：时空图$G^{front}$。跨相邻帧的、高度重叠的建议对象通过有向边连接。我们用蓝色框绘制一些示例轨迹，方向指示了时间箭头。</p>
</blockquote>
<p>虽然相似图甚至可以捕捉到任何两个建议对象之间的长期依赖关系，但它不会捕捉对象之间的相对空间关系以及状态变化的顺序。为了对对象之间的这些空间和时间关系进行编码，我们建议使用时空图，其中，在空间和时间中相近的对象连接在一起。</p>
<p>给定帧$t$中的一个建议对象，我们计算这个对象边界框与帧$t+1$中所有其他对象边界框之间的<u>Intersection Over Unions</u>（IoUs）的值。我们将帧$t$的对象$i$和帧$t+1$的对象$j$之间的IoU表示为$\sigma_{ij}$。如果$\sigma_{ij}$大于0，我们将使用一条有向边$i→j$来将对象$i$连接到对象$j$。给边赋值后，我们将连接到每个建议对象$i$的边的值总和归一化为1，通过以下公式：</p>
<p>$G<sup>{front}_{ij}=\dfrac{\sigma_{ij}}{\sum</sup>N_{j=1}\sigma_{ij}}\tag{3}$</p>
<p>其中，$G^{front}$是时空图的邻接矩阵。我们将图4中的一些建议对象和轨迹可视化。</p>
<p>除了构建将帧$t$的对象连接到帧$t+1$的对象的前向图之外，我们还以类似的方式构建了一个后向图，它将帧$t+1$的对象连接到帧$t$。我们将该后向图的邻接矩阵表示为$G<sup>{back}$。具体地说，对于重叠的帧$t$中的对象$i$和帧$t+1$中的对象$j$，我们构造边缘$i←j$并根据IoU值给$G</sup>{back}_{ji}$赋值。通过以双向的方式构造时空图，我们可以获得更丰富的结构信息，并在图形卷积期间增加传播领域的数量。</p>
<h2 id="convolutions-on-graphs"><a class="header-anchor" href="#convolutions-on-graphs"> </a>Convolutions on Graphs</h2>
<p>为了在图上进行推理，我们应用了文献[19]中提出的图形卷积网络（GCNs）。不同于在局部规则网格上运行的标准卷积，图形卷积允许我们基于由图形关系定义的邻居来计算节点的响应。因此，执行图形卷积等同于在图形内执行消息传递。GCNs的输出是每个对象节点的更新的特征，可以将它们聚合到一起以进行视频分类。我们可以将图形卷积的一层表示为：</p>
<p>$$Z=GXW\tag{4}$$</p>
<p>其中，$G$代表我们引入的邻接图之一（$G<sup>{sim}$，$G</sup>{front}$或$G^{back}$），它具有$N×N$的维度；$X$代表对象节点的输入特征，有$N×d$的维度；$W$是这个层的权重矩阵，在我们的例子中，它有$d×d$的维度。因此，一个图形卷积层的输出仍是$N×d$的维度。图形卷积运算可以堆叠成多个层。在每层图形卷积之后，特征$Z$被转发到下一层之前，我们应用两个非线性函数，包括<u>Layer Normalization</u>和ReLU。</p>
<p>要在GCNs中组合多个图形，我们可以简单地扩展方程$(4)$为：</p>
<p>$$Z=\sum\limits_iG_iXW_i\tag{5}$$</p>
<p>其中，$G_i$表示不同类型的图，并且不同图的权重$W_i$是不共享的。注意，通过这种方式，GCN的每个隐藏层会通过不同图形的关系来更新。但是，我们发现，3个图（$G<sup>{sim}$，$G</sup>{front}$和$G^{back}$）与方程$(5)$的直接组合，和使用单个相似图的情况相比，实际上损害了性能。</p>
<p>原因是，我们的相似图$G<sup>{sim}$包含了可学习的参数（方程$(1)$），并且需要反向传播以进行更新，而其他两个图不需要学习。在每个GCN层中，将这些图结合到一起会增加优化难度。因此，我们创建了图形卷积网络的两个分支，最终只把两个GCN的结果组合起来：一个GCN采取方程$(4)$和$G</sup>{sim}$，另一个GCN采取方程$(5)$和$G<sup>{front}$、$G</sup>{back}$。GCN的这两个分支分别对$L$个层执行卷积，最终的层特征被加到一起，维度为$N×d$。</p>
<p>**视频分类。**如图2所示，图形卷积之后的更新的特征被转发到一个平均池化层，该层计算所有建议特征的平均值并引向一个$1×d$维的表示。除了GCN特征以外，我们还在整个视频级别的表示上执行平均池化，获得另一个$1×d$维的全局特征。这两个特征被连接在一起以进行视频分类。</p>
<h2 id="experiments"><a class="header-anchor" href="#experiments"> </a>Experiments</h2>
<p>我们在最近的两个具有挑战性的数据集上进行实验：Charades和Something-Something。我们首先介绍我们方法的实现细节，然后介绍对在这些数据集上的结果的评估。</p>
<p>**训练。**我们骨干模型的训练包括下列两个不同数据集的预训练。该模型首先作为一个带有ImageNet数据集的2D卷积网络进行预训练，然后膨胀为3D卷积网络（即，I3D）。然后我们遵循文献[58]中的对更长的片段（大约5秒的视频）的同一个训练模式，对<u>动力学动作识别数据集 Kinetics action recognition dataset</u>上的3D卷积网络进行微调。在这样的初始化之后，现在我们介绍如何进一步微调我们在目标数据集上的网络，如下所示。</p>
<p>如表1所示，我们的网络需要32个视频帧作为输入。以6fps的帧率对这32个视频帧进行采样，因此视频剪辑的时间长度约为5秒。输入的空间尺寸为$224×224$。跟随文献[89]，输入帧是从一个随机缩放的视频（短边长度在$[256.320]$之间采样）随机裁剪出的。为了减少GCN参数的数列，我们在I3D基准模型之上增加了一个$1×1×1$的卷积层，从而将输出通道数量$d$从2048减少到512。由于Charades和Something-Something数据集的视频帧数量相近，我们对两个数据集采用相同的学习率策略。</p>
<p>我们的基准I3D模型使用有4个GPU的机器进行训练。训练期间的批大小为8个clip。注意，我们在训练期间将所有批归一化层的参数都冻结了。我们的模型总共训练了100K轮，在前90K轮中学习率为0.00125，后10K轮中学习率减少到原来的1/10。<u>dropout</u>应用于最后一个全局池化层，比率为0.3。</p>
<p>我们将我们的图形卷积网络的层数设置为3。前两个层是随机初始化的，最后一个层初始化为0，受文献[91]的启发。为了将GCN和I3D骨干模型一起训练，我们建议应用阶段性训练。我们首先对上面提到的I3D模型进行微调，然后在最终的卷积特征之上应用RoIAlign和GCN，如图2所示。我们修复了I3D特征，并使用和训练骨干模型相同的学习率策略来训练GCN。然后我们将I3D和GCN一起端到端地训练，以便在学习率降低的情况下再进行30K轮。</p>
<p>*任务的特定设置。*我们在训练Charades和Something-Something时应用不同的损失函数。对于Something-Something数据集，我们可以简单地应用softmax损失函数。对于Charades，我们应用<u>binary sigmoid</u>损失函数来处理多标签属性。我们还在两个不同的数据集中使用RPN提取了不同数量的对象边界框。对Charades，情景更混乱，我们为每帧提取50个建议对象。但是，对于Something-Something，视频帧的中心通常只有一个或两个对象。我们发现对Something-Something，每帧提取10个建议对象是足够的。</p>
<p>**推理。**在推理过程中，我们执行如文献[89,58]中的空间完全卷积。每个视频帧的短边被缩放到256，同时保持宽高比。在测试一整个视频的期间，我们根据两个数据集的平均视频长度，为Charades采样10个clip而为Something-Something采样2个clip。来自多个clip的分数被最大池化聚合到一起。</p>
<p><img src="/2019/04/30/paper-reading-videos-as-space-time-region-graphes/table-2.png" alt="表2"></p>
<blockquote>
<p>表2：在Charades上的<u>Ablations</u>。我们显示平均精度的均值（mAP%）。</p>
</blockquote>
<h3 id="experiments-on-charades"><a class="header-anchor" href="#experiments-on-charades"> </a>Experiments on Charades</h3>
<p>在Charades实验中，在官方分割后，我们使用8K个训练视频来训练我们的模型并对1.8K个验证视频进行测试。平均视频时长约为30秒。有157个动作类，多个动作可以同时发生。</p>
<p>**每个图有多少帮助？**我们首先使用ResNet-50 I3D的骨干模型来对我们框架的每个组件进行分析，如表2(a)所示。我们首先显示，在没有任何建议提取和图形卷积的情况下，I3D基准模型的结果在验证集上是31.8%mAP。</p>
<p>此基准的一个简单扩展是，使用RPN获取区域建议，提取每个建议的功能，并对其进行平均池化，作为一个额外的特征。我们将视频级特征和建议特征连接到一起，进行分类。但是，使用这种方法，我们只能获得0.3%的提升。因此，建议特征的简单聚合并没有多大的帮助。</p>
<p>然后，我们通过应用各自具有相似图和时空图的GCN，来执行评估。我们观察到，仅具有时空图的GCN模型可以在基准模型之上获得2.4%的提升，并达到34.2%。使用相似图，我们可以实现35.0%的更好的性能。通过将两个图组合到一起并训练具有多个关系的GCN，我们的方法实现了36.2%的mAP。这比基准显著提高了4.4%。</p>
<p>**建议数量的稳健性。**我们还分析了RPN生成的建议对象数量如何影响我们的方法。注意，每个视频帧提取50个对象时，我们的方法达到36.2%。如果我们每帧提取25（100）个建议对象，我们方法的mAP是35.9%（36.1%）。因此我们方法的性能实际上非常稳定，不管RPN怎么变化。</p>
<p>**模型的复杂性。**鉴于性能的大幅提升，GCN相对于基准模型的额外计算成本实际上非常小。在Charades数据集中，我们的图标是基于每个视频的800个对象节点（有16个输出帧，每帧有50个建议对象）来定义的。基准I3D模型的FLOP是$153×10<sup>9$，我们的模型（I3D+GCN）的总的FLOP是$158×10</sup>9$。因此，FLOP仅增加约3%。</p>
<p>**与非局部网络相比。**相关工作之一是最近提出的非局部神经网络，他们建议在不同的特征图层上执行时空推理。如表2(b)所示，非局部操作相较基准模型而言有1.7%的提升，而我们的模型又比非局部网络提高了2.7%。这两种方法实际上是相互补充的。通过用非局部网络代替I3D骨干模型，我们有1.3%的提升，达到了37.5%。</p>
<p><img src="/2019/04/30/paper-reading-videos-as-space-time-region-graphes/figure-5.png" alt="图5"></p>
<blockquote>
<p>图5：错误分析。我们将我们的方法与三种不同属性的基准I3D方法进行比较。当动作是序列的一部分，涉及与对象的交互并且有较高的姿势变化幅度时，我们的方法有显著的提升。</p>
</blockquote>
<p>**错误分析。**鉴于这一重大改进，我们还希望了解在什么情况下，我们的方法最能改进基准。根据文献[11]中设置的属性，我们在图5中显示了3种不同的情况，其中我们的方法比基准获得了更大的收益。具体地说，对Charades中的每个视频，除了动作标签之外，它还标有不同的属性（比如，动作是否按顺序发生？）</p>
<p>*序列的一部分？*这个属性指定动作类别是否是动作序列的一部分。比如，“拿着杯子”然后“坐下”通常是一系列动作，而“跑步”通常是孤立地发生的。如图5中的左图所示，当动作是动作序列的一部分时，I3D基准模型显著失败，而我们的方法更为稳定。如果一个动作没有孤立地发生，我们实际上比基准提升了5%。</p>
<p>*姿势差异。*这个属性是通过对一个动作类别中的任何两个姿势之间的<u>Procrustes</u>距离求平均值来计算的。如果平均距离很大，则意味着在动作中姿势会有很大变化。如图5的中间图所示，我们可以看到，当姿势差异较小时，我们的方法与基准具有相似的性能。然而随着姿势变化在动作中变得更大（从0.68到0.73），基准的性能再次急剧下降，而我们的曲线斜率要小得多。随着姿势差异达到0.83，两种方法的表现都有所改善，我们的方法大约比基准提升8%~9%。</p>
<p>*涉及对象？*该属性指定对象是否参与操作。例如，“从杯子里喝水”涉及杯子这个对象，而“跑步”不需要与对象互动。如图5中的右图所示，当动作需要与对象交互时，我们可以看到基准的表现更差。有趣的是，当涉及对象时，我们的方法实际上表现稍好一些。</p>
<p>简单地总结一下，在对较长的一系列动作和需要与对象交互的动作建模时，我们的方法是比较好的。我们的方法也对姿势的变化有更好的健壮性，能够利用姿势的运动。</p>
<p><img src="/2019/04/30/paper-reading-videos-as-space-time-region-graphes/table-3.png" alt="表3"></p>
<blockquote>
<p>表3：Charades数据集中的分类mAP(%)。NL是非局部的缩写。</p>
</blockquote>
<p>**用更大的骨干训练。**除了ResNet-50骨干架构，我们还在文献[58]中应用的更大的骨干模型上验证了我们的方法。该骨干在3个方面比我们的基准大：（i）该骨干是基于ResNet-101架构的，而非ResNet-50；（ii）该骨干不使用$224×224$的输入大小，而使用$288×288$；（iii）该骨干网不是采用6fps对32帧采样，而是使用24fps、128帧作为输入，进行更密集的采样。注意，我们的基准模型和此ResNet-101主干的时间输出维数仍然相同（维数为16）。在对骨干架构的所有修改之后，FLOP是我们的ResNet-50基准模型的3倍。</p>
<p>我们将结果与所有最先进的方法在表3中一起展示。具有ResNet-101骨干的非局部网络实现了37.5%的mAP。实际上，我们可以使用我们的方法通过更小的ResNet-50骨干（有大约1/3的FLOP）来实现相同的性能。通过应用有ResNet-101骨干模型的我们的方法，我们的方法（I3D+GCN）仍能提供3.6%的提升，从而达到39.1%。这是另一项证据，它显示我们的方法只是通过增加空间输入和ConvNets的深度来建模出非常不同的东西。通过将我们的操作与非局部操作结合，我们达到了39.7%的最终性能。</p>
<p><img src="/2019/04/30/paper-reading-videos-as-space-time-region-graphes/table-4.png" alt="表4"></p>
<blockquote>
<p>表4：Something-Something数据集中的分类准确度（%）。NL是非局部的缩写。</p>
</blockquote>
<h3 id="experiments-on-something-something"><a class="header-anchor" href="#experiments-on-something-something"> </a>Experiments on Something-Something</h3>
<p>在Something-Something数据集中，有86K个训练视频，大约12K个验证视频和11K个测试视频。每个视频的持续时间从3秒到6秒不等。类别总数为174。Something-Something数据集中的数据与Charades数据集非常不同。在Charades数据集中，大多数动作都是由中间人在杂乱的室内场景中执行的。但是，在Something-Something数据集中，所有视频都是以对象为中心的，通常只有一两只手与中心对象进行交互。在大多数情况下，Something-Something数据集中的背景也非常干净。</p>
<p>我们在表4中报告了结果。I3D基准方法在top-1精度上达到了41.6%而在top-5精度上达到了72.2%。通过使用I3D骨干应用我们的方法（I3D+GCN），我们在top-1精度上实现了1.7%的提升。我们观察到在top-1精度上的提升并不像我们在Charades数据集上实现的那么大。原因主要是视频已经利用在帧中心的对象很好地校准过了。有趣的是，我们在top-5上，与top-1相比，仍有相对较大的2.9%的提升。我们还研究了每个子图的性能。如果我们只使用时空图，我们会获得42.8%的top-1精度。仅使用相似图时，我们获得42.7%的精度。</p>
<p>我们还将我们的方法与非局部网络结合起来。如表4所示，非局部I3D方法在top-1精度上达到44.4%。通过将我们的方法与非局部网络结合，我们在top-1精度上实现了另外1.7%的提升，这导致最先进的结果为46.1%。我们还通过提交到官方网站来在测试集上测试我们的最终模型。通过使用单一RGB模型，我们在排行榜中获得了45.0%的最佳结果。</p>
<h2 id="conclusions"><a class="header-anchor" href="#conclusions"> </a>Conclusions</h2>
<p>我们提出了一种新颖的基于图的网络，来对视频中的长程关系建模，以进行动作识别。与最先进的技术相比，较大的性能提升证明了我们框架的有效性。但更重要的是，我们的错误分析显示了我们的模型在捕捉对象交互、姿势变化和序列中的动作等方面做得更好。这表明我们的模型不仅在视频分类，还在包括视频中的检测和跟踪等的变体任务之中具有很大的潜力。</p>
<h1 id="论文笔记"><a class="header-anchor" href="#论文笔记"> </a>论文笔记</h1>
<p>（未完待续）</p>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
            <a href="/tags/视频分类/" rel="tag"># 视频分类</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/19/shortest-distance-of-cities/" rel="next" title="求城市旅游的最短路径">
                <i class="fa fa-chevron-left"></i> 求城市旅游的最短路径
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        
          <ul class="sidebar-nav motion-element">
            <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
              文章目录
            </li>
            <li class="sidebar-nav-overview" data-target="site-overview-wrap">
              站点概览
            </li>
          </ul>
        
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/uploads/avatar.png"
                alt="yuyu" />
            
              <p class="site-author-name" itemprop="name">yuyu</p>
              <p class="site-description motion-element" itemprop="description">鱼鱼爱吃鱼</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">32</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">7</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">18</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/yuyuforest" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:yumlin16@outlook.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://blog.csdn.net/squirrelyuyu" target="_blank" title="CSDN"><i class="fa fa-fw fa-cuttlefish"></i>CSDN</a>
                  
                </span>
              
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://icytown.com" title="MegaShow" target="_blank">MegaShow</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.zzfly.net/" title="Rytia" target="_blank">Rytia</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </section>

      
        
        <!--noindex-->
          <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
            <div class="post-toc">

              
                
              

              
                <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#原文翻译"><span class="nav-number">1.</span> <span class="nav-text"> 原文翻译</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#abstract"><span class="nav-number">1.1.</span> <span class="nav-text"> Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction"><span class="nav-number">1.2.</span> <span class="nav-text"> Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#related-work"><span class="nav-number">1.3.</span> <span class="nav-text"> Related Work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#overview"><span class="nav-number">1.4.</span> <span class="nav-text"> Overview</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#graph-representation-in-videos"><span class="nav-number">1.5.</span> <span class="nav-text"> Graph Representation in Videos</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#video-representation"><span class="nav-number">1.5.1.</span> <span class="nav-text"> Video Representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#similarity-graph"><span class="nav-number">1.5.2.</span> <span class="nav-text"> Similarity Graph</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spatial-temporal-graph"><span class="nav-number">1.5.3.</span> <span class="nav-text"> Spatial-Temporal Graph</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#convolutions-on-graphs"><span class="nav-number">1.6.</span> <span class="nav-text"> Convolutions on Graphs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#experiments"><span class="nav-number">1.7.</span> <span class="nav-text"> Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#experiments-on-charades"><span class="nav-number">1.7.1.</span> <span class="nav-text"> Experiments on Charades</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#experiments-on-something-something"><span class="nav-number">1.7.2.</span> <span class="nav-text"> Experiments on Something-Something</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#conclusions"><span class="nav-number">1.8.</span> <span class="nav-text"> Conclusions</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#论文笔记"><span class="nav-number">2.</span> <span class="nav-text"> 论文笔记</span></a></li></ol></div>
              

            </div>
          </section>
        <!--/noindex-->
        
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yuyu</span>

  

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动 v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Muse</a> v6.4.2</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.4.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.4.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.4.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.2"></script>



  



  










  





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  

  

  

  

</body>
</html>
